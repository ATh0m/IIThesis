\chapter{Wstęp}

\section{Sztuczne sieci neuronowe}

Sztuczne sieci neuronowe mają obecnie bardzo mocno ugruntowaną pozycję szczególnie w dziedzinie problemów związanych z analizą i przetwarzaniem obrazów. Pomimo, iż nie jest to nowy pomysł, dopiero ostatni wzrost w wydajności komputerów pozwolił na ich praktyczne zastosowanie. Z matematycznego punktu widzenia są to sparametryzowane nieliniowe funkcje o pewnej ustalonej strukturze. Składają się z prostych elementów zwanych neuronami, a one natomiast są pogrupowane w warstwy. Połączenia między warstwami definiują przepływ danych. 'Nauka' sieci neuronowych polega na optymalizacji pewnej funkcji straty, czyli wyznaczeniu takich parametrów, żeby osiągnąć minimalny koszt. Do tego celu często korzysta się z metod opartych na Stochastic Gradient Descent, a przy wybranej strukturze można w efektywny sposób zastosować algorytm propagacji wstecznej. W dalszej części pracy będę używał prostszej nazwy - sieci neuronowe. Przykładowa architektura sieci neuronowych jest zaprezentowana na wykresie \ref{fig:neural_nets}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\textwidth]{images/neural_nets}
    \caption{Przykładowy model sieci neuronowej}
    \label{fig:neural_nets}
\end{figure}

\section{Uczenie nadzorowane i bez nadzoru}

Są to dwa różne rodzaje problemów z dziedziny uczenia maszynowego. W przypadku uczenia nadzorowanego wiemy jaką chcemy uzyskać odpowiedź dla danego wejścia i próbujemy tak dobrać parametry modelu, żeby odpowiadał on z jak największą poprawnością. Przykładowymi zadaniami tego rodzaju są regresja oraz klasyfikacja, która nawiasem mówiąc może zostać uznana za problem regresji, ale na dyskretnym, skończonym zbiorze.

Z grubsza odwrotnie jest w sytuacji bez nadzoru. Tam dla danych wejściowych nie znamy interesującej nas odpowiedzi i staramy się tak zbudować model, żeby był on w stanie sam ją wydobyć. Zazwyczaj interesuje nas rozwiązanie takich problemów jak klasteryzacja, redukcja wymiarowości czy wydajne kodowanie danych. Jednym z modeli wykorzystywanym w tej klasie zadań jest właśnie autoenkoder.

\section{Autoenkoder}

Jest to jeden z rodzai sieci neuronowych, służący do znajdowania wydajnej reprezentacji danych, co jak wspomniałem wcześniej jest przykładem nauki bez nadzoru. Myślę, że mogę go określić mianem nieliniowej alternatywy dla klasycznej statystycznej metody analizy głównych składowych (PCA, ang. principal component analysis). W autoenkoderach można wyróżnić dwie połączone ze sobą części zwane enkoderem i dekoderem. Zadaniem enkodera jest wyprodukowanie właśnie tej reprezentacji, podczas gdy dekoder służy do odtworzenia z niej oryginalnej postaci. Zależy nam na tym, żeby wyjście było w jakimś sensie jak najbardziej podobne do wejścia przy zachowaniu odpowiednio małej wymiarowości kodowania. Jest to pewnego rodzaju balansowanie pomiędzy ilością informacji, na których przepływ się zgadzamy, a ich jakością. W szczególności można pomyśleć o takiej patologicznej sytuacji jak ustalenie rozmiaru reprezentacji równej rozmiarowi danych, co prawdopodobnie będzie skutkować idealnymi rekonstrukcjami, ale przy okazji zerową wiedzą płynącą z takiego modelu. Analogicznie można rozważyć przypadek kiedy obszar kodowania jest za mały. Model w takiej sytuacji skupi się jedynie na przekazaniu wyłącznie trywialnych cech, żeby mimo wszystko jakkolwiek odtworzyć dane. Poglądowy schemat budowy znajduje się na rysunku \ref{fig:autoenc}.

W przypadku obrazów na funkcję straty często wybierany jest błąd średniokwadratowy (MSE, ang. Mean Squared Error).

\begin{equation}
\mathrm { MSE } = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \left( Y _ { i } - \hat { Y } _ { i } \right) ^ { 2 }
\end{equation}
gdzie:
\begin{conditions}
    Y_i             &  oryginalne dane \\
    \hat{Y}_{i}     &  zrekonstruowane dane
\end{conditions}

Może wymagającym sprostowania jest fakt, iż mimo mówimy jakie oczekujemy wyjście z modelu, w sensie rekonstrukcji, to dalej jest to problem typu nauki bez nadzoru, ponieważ rzeczywistą szukaną przez nas wiedzą jest umiejętność kodowania danych i osobno ich dekodowania. Chciałbym w tym miejscu zastanowić się co nam to zyskujemy na tym. Już w tym momencie zaletą jest oczywiście redukcja wymiarowości, która ułatwia późniejszą analizę danych oraz pozwala na ich kompresję i ewentualne odszumianie. Dodatkowo wydaje się, że mając wytrenowany dekoder, na konkretnej rodzinie danych, moglibyśmy zaaplikować do niego dowolne kodowanie i w ten sposób może otrzymać jakiś wynik pochodzący z oryginalnej dziedziny, czyli wykorzystać go jako generator nowych próbek. Niestety ten pomysł ma taki problem, że podczas nauki nie ma żadnej kontroli nad tym jak dane zostaną rozrzucone w przestrzeni, która notabene jest nieskończona (z ograniczeniem do pojemności liczb zmiennoprzecinkowych). W takim razie nie wiemy jaka podprzestrzeń odpowiada tym kodowaniom, które są sensownie dekodowane, co jest potrzebne przy generowaniu, a dodatkowo jeśli bałaby nieciągła, to niemożliwe byłoby interpolowanie pomiędzy próbkami. Znaczącym problemem jest natomiast to, że kodowanie jako wektor liczb jest bardzo precyzyjną informacją, co może doprowadzić do przeuczenia modelu i błędne działania na danych pochodzących z poza zbioru treningowego.

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{images/autoenc}
    \caption{Architektura autoenkodera}
    \label{fig:autoenc}
\end{figure}

\section{Variational autoencoder}

W przypadku podstawowych autoenkoder'ów jednym z problemów jest brak kontroli nad tym jak dane są rozrzucane w przestrzeni. O ile sam model może bardzo dobrze rekonstruować dane, to same ich reprezentacja w przestrzeni może nie być ciągła, tj. pewne obszary nigdy nie były używane i dekoder nie za bardzo wie co zrobić, jeśli wylosujemy wektor z tego regionu. Jest to problemem przy generowaniu nowych danych, bądź interpolowaniu między nimi. W innych natomiast miejscach dane mogą zostać poukładane zbyt ciasno, co skutkować będzie błędną rekonstrukcją na niewidzianych danych. Próbą rozwiązania tych problemów jest właśnie Vaeriational Autoencoder.

Variational autoencoder rozszerza założenia o wprowadzenie modelowania rozkładu prawdopodobieństwa dla reprezentacji ukrytej. Czyli teraz encoder nie produkuje pojedyńczego wektora, ale dwa wektory interpretowane odpowiednio jako średnia $\mu$ oraz wariancja $\sigma$ dla rozkładu Gaussa. Następnie z tych parametrów formowany jest wektor, gdzie $i$-ty element powstał z $i$-tej pary $\mu$, $\sigma$.Dopiero taki wektor trafia do decodera, a z niego rekonstruowany jest wejściowa dana. Schemat przedstawiony jest na rysunku \ref{fig:vae_model}

Takie generowanie znaczy, że nawet dla tego samego wejścia i podczas gdy średnie oraz odchylenie zostaną takie same, to rzeczywiste kodowanie będzie się mimo wszystko różnić właśnie ze względu na to próbkowanie. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\textwidth]{images/vae_model}
    \caption{Architecture of Variational Autoencoder}
    \label{fig:vae_model}
\end{figure}

Intuicyjnie wektor średnich oznacza, gdzie zakodowana zmienna powinna być wycentrowana, natomiast odchylenie kontroluje obszar, wokół którego kodowanie może się różnić. Podczas gdy kodowanie jest losowane z wewnątrz koła, dekoder uczy się, że nie jedynie pojedynczy punkt z utajonej przestrzeni odnosi się do próbki, ale wszystkie punkty znajdujące się w pobliżu odnoszą się również do tego samego. To pozwala dekoderowi dekodować nie tylko pojedyncze, specyficzne reprezentacje (zostawiając przestrzeń nieciągłą), ale też te trochę się różniące, ponieważ narażony jest na szereg zmian kodowania tego samego wejścia podczas treningu. Rysunek \ref{fig:vae_vs_stand}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{images/vae_vs_stand}
    \caption{}
    \label{fig:vae_vs_stand}
\end{figure}

Model jest teraz wystawiony na pewien stopień lokalnej zmienności przez zmianę kodowania pojedynczej próbki, co skutkuje ciągłą przestrzenią w reprezentacji ukrytej dla lokalnego sąsiedztwa. Jednak na razie nie ma ograniczeń dla wartości jakie mogą przyjmować wektory $\mu$ i $\sigma$, przez co enkoder może nauczyć się generować bardzo różne $\mu$ dla różnych klas i grupować je minimalizując $\sigma$, upewniając się, że samo kodowanie nie różni się zbytnio dla tej samej próbki. W szczególności $\sigma$ może wynosić 0, przez co wrócimy do podstawowego modelu. To oczywiście pozwala dekoderowi na skuteczną rekonstrukcję, ale jedynie danych treningowych. Przykład przedstawiony jest na rysunku \ref{fig:vae_nolimits}. To co byśmy idealnie chcieli to kodowanie, gdzie grupy są blisko siebie, ale mimo wszystko są dobrze separowalne. Pozwoliłoby nam to na gładką interpolacje między próbkami i konstruowanie nowych danych.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{images/vae_nolimits}
    \caption{Architecture of Variational Autoencoder}
    \label{fig:vae_nolimits}
\end{figure}

Żeby to osiągnąć wzbogacimy funkcję straty o koszt the Kullback–Leibler divergence. KL divergence mierzy po prostu różnicę pomiędzy dwoma rozkładami prawdopodobieństwa. Minimalizowanie go oznacza optymalizowania parametrów rozkładu ($\mu$ i $\sigma$), tak żeby jak najbardzieć przypominały celowy rozkład. W przypadku VAE będzie to standardowy rozkład normalny ($\mu = 0$,  $\sigma = 1$).

$$
\mathrm { KLD } = \sum _ { i = 1 } ^ { n } \sigma _ { i } ^ { 2 } + \mu _ { i } ^ { 2 } - \log \left( \sigma _ { i } \right) - 1
$$

\subsection{Magia, która za tym stoi}

To co do tej pory napisałem to były w większości intuicje i nasze zachcianki. Chciałbym teraz przedstawić jak to wygląda od strony matematycznej.

\add[inline]{Dokończyć matmę}

%\section{Convolutional VAE}
%
%Jest to rozszerzenie poprzedniego modelu, w którym dodatkowo stosujemy warstwy splotowe. Szczególnie w przypadku obrazów pozwala to na zwiększenie rozmiaru danych wejściowych przez zmniejszenie ilości parametrów w stosunku do warstw fully-connected oraz wykryciu na wstępie jakiś prostych cech, przez co w reprezentacji mogą znajdowac się bardziej abstrakcyjne rzeczy. 

\section{Deep feature consistent variational auto-encoder}

W autoenkoderze wariacyjnym jednym ze składników kosztu jest błąd rekonstrukcji. Standardowo funkcja ta oparta jest na porównywaniu wartości odpowiadającym sobie pikselom z oryginalnego obrazu i tego zrekonstruowanego. Często przyczynia się to do uśredniania wartości pikseli przez co wyjściowy obraz nie jest wyraźny. Przyczynie sie do tego między innymi fakt, że taki liczenie błędu jest bardzo czułe na chociażby przesunięcie. Oryginalny obraz i ten lekko przesunięty mimo, iż nie różnią się prawie wogóle perceptualnie dla człowieka, to ich koszt MSE jest bardzo duży. Uśrednianie wartości jest sposobem na optymalizowanie tego problemu. Wynika z tego, że raczej chcielibyśmy patrzeć na to co znajduje się na obrazkach, niż porównywać poszczególne piksele.

I w tym właśnie miejscu pojawia się Deep feature consistent. Opiera się ona na wykorzystaniu wcześniej już wyuczonej sieci splotowej. W takich sieciach coraz głębsze filtry odpowiadają za rozpoznawanie coraz to bardziej abstrakcyjnych cech. Będziemy teraz myśleć, że dwa obrazy są podobne, jeśli maja podobne wartości aktywacji w warstwach ukrytych tej sieci. Dzięki takiemu patrzeniu na różnice dwa obrazy powinny być podobne nawet jeśli zostaną poddane takim operacjom jak m.in przesunięcie, rotacja, co stanowi duży problem dla kosztów typu pixel-to-pixel. Bład ten jest okreslany mianem perceptual loss.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{images/dfc_vae}
    \caption{Architecture of Variational Autoencoder}
    \label{fig:dfc_vae}
\end{figure}
